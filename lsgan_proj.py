# -*- coding: utf-8 -*-
"""LSGAN_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1smz7GmLKrCtBFiVgnm9JOupJR5qlRbdY
"""

import os
import numpy as np
import math

import torchvision.transforms as transforms
from torchvision.utils import save_image

from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch

#os.makedirs("images", exist_ok=True)

n_epochs = 20
batch_size = 64
lr = 0.0002
b1 = 0.5
b2 = 0.999
n_cpu = 8
latent_dim = 100
img_size = 32
channels = 1
sample_interval = 1000

cuda = True if torch.cuda.is_available() else False

def weights_init_normal(m):
  classname = m.__class__.__name__
  if classname.find("Conv") != -1:
    torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
  elif classname.find("BatchNorm") != -1:
    torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
    torch.nn.init.constant_(m.bias.data, 0.0)

'''# Wasserstein Loss
def Wasserstein_loss(discriminator, real_samples, fake_samples):
    real_output = discriminator(real_samples)
    fake_output = discriminator(fake_samples)

    w_loss = torch.mean(fake_output) - torch.mean(real_output)

    return w_loss

# Gradient Penalty
def gradient_penalty(discriminator, real_samples, fake_samples, device):
    batch_size = real_samples.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1).to(device)
    interpolated_samples = alpha * real_samples + (1 - alpha) * fake_samples
    interpolated_samples.requires_grad_(True)

    # Calculate interpolated samples output
    interpolated_output = discriminator(interpolated_samples)

    # Compute gradients of the output with respect to the interpolated samples
    gradients = torch.autograd.grad(outputs=interpolated_output, inputs=interpolated_samples,
                                    grad_outputs=torch.ones(interpolated_output.size()).to(device),
                                    create_graph=True, retain_graph=True)[0]

    # Compute gradient penalty
    gradient_penalty = torch.mean((gradients.norm(2, dim=1) - 1) ** 2)

    return gradient_penalty'''

class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()

    self.init_size = img_size // 4
    self.l1 = nn.Sequential(nn.Linear(latent_dim, 128*self.init_size**2))

    self.conv_blocks = nn.Sequential(
        nn.Upsample(scale_factor=2),
        nn.Conv2d(128, 128, 3, stride=1, padding=1),
        nn.BatchNorm2d(128, 0.8),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Upsample(scale_factor=2),
        nn.Conv2d(128, 64, 3, stride=1, padding=1),
        nn.BatchNorm2d(64, 0.8),
        nn.LeakyReLU(0.2, inplace=True),
        nn.Conv2d(64, channels, 3, stride=1, padding=1),
        nn.Tanh(),
    )

  def forward(self, z):
    out = self.l1(z)
    out = out.view(out.shape[0], 128, self.init_size, self.init_size)
    img = self.conv_blocks(out)
    return img

class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()

    def discriminator_block(in_filters, out_filters, bn=True):
      block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]
      if bn:
        block.append(nn.BatchNorm2d(out_filters, 0.8))
      return block

    self.model = nn.Sequential(
        *discriminator_block(channels, 16, bn=False),
        *discriminator_block(16, 32),
        *discriminator_block(32, 64),
        *discriminator_block(64, 128),
    )

    # The height and width of downsampled image
    ds_size = img_size // 2 ** 4
    self.adv_layer = nn.Linear(128 * ds_size ** 2, 1)

  def forward(self, img):
    out = self.model(img)
    out = out.view(out.shape[0], -1)
    validity = self.adv_layer(out)
    return validity

# Loss Function
#adversarial_loss = torch.nn.SmoothL1Loss()

mse_loss = torch.nn.MSELoss()
#wasserstein_loss = WassersteinLoss()

# Initialize generator and discriminator
generator = Generator()
discriminator = Discriminator()

if cuda:
    generator.cuda()
    discriminator.cuda()
    #adversarial_loss.cuda()
    mse_loss.cuda()

# Initialize weights
generator.apply(weights_init_normal)
discriminator.apply(weights_init_normal)

# Configure data loader
os.makedirs("../../data/mnist", exist_ok=True)
dataloader = torch.utils.data.DataLoader(
    datasets.MNIST(
        "../../data/mnist",
        train=True,
        download=True,
        transform=transforms.Compose(
            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        ),
    ),
    batch_size=batch_size,
    shuffle=True,
)

# Optimizers
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))

Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

for i, (imgs, _) in enumerate(dataloader):
  print(imgs.shape)
  print(_)
  valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)
  fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)
  print(valid.shape)
  print(fake.shape)
  real_imgs = Variable(imgs.type(Tensor))
  print(real_imgs.shape)
  print(real_imgs.type)

  break

alpha = 0.5
beta = 0.5

# Commented out IPython magic to ensure Python compatibility.
# ----------
#  Training
# ----------

for epoch in range(n_epochs):
    for i, (imgs, _) in enumerate(dataloader):

        # Adversarial ground truths
        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)
        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------

        optimizer_G.zero_grad()

        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))

        # Generate a batch of images
        gen_imgs = generator(z)
        #print(gen_imgs.shape)

        d = discriminator(gen_imgs)
        d_real = discriminator(real_imgs)
        #print(d)
        # Loss measures generator's ability to fool the discriminator
        #g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        g_loss_mse = mse_loss(discriminator(gen_imgs),valid)
        g_loss_wasserstein =  -torch.mean(d)
        g_loss = alpha * g_loss_mse + beta * g_loss_wasserstein

        g_loss.backward(retain_graph=True)
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        #real_loss = adversarial_loss(discriminator(real_imgs), valid)
        #fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        real_loss_mse = mse_loss(discriminator(real_imgs), valid)
        fake_loss_mse = mse_loss(discriminator(gen_imgs.detach()), fake)
        d_loss_wasserstein = torch.mean(d) - torch.mean(d_real)

        #d_loss = 0.5 * (real_loss + fake_loss)
        d_loss = (alpha * (real_loss_mse + fake_loss_mse) + beta * d_loss_wasserstein)
        d_loss.backward()
        optimizer_D.step()

        batches_done = epoch * len(dataloader) + i
        if batches_done % sample_interval == 0:
            save_image(gen_imgs.data[:25], "/content/drive/MyDrive/GAN/MNIST_MSEW_0.5_0.5/%d.png" % batches_done, nrow=5, normalize=True)

    print(
            "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]"
#             % (epoch+1, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())
        )

